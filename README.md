# Dynamic Weight Models: Bridging GLM and Neural Networks
What if logistic regression could work more like a neural network? Traditional logistic regression uses fixed weights, which are great for linear relationships but fall short when it comes to capturing complex patterns. To solve this, I developed Dynamic Weight Logistic Regression (DWLR), where weights dynamically adjust based on input features. These weights are calculated using activation functions like sigmoid, ReLU, tanh, and softplus. Imagine each feature getting its own flexible, customized weight - like tailoring a suit to fit every individual data point perfectly.
I evaluated this method by benchmarking it against standard models, including logistic regression, XGBoost, LightGBM, Random Forest, and GAM. The proposed approach consistently outperformed these models, showing promising potential.
The next step was building the Dynamic Interaction Neural Network (DINN), taking dynamic weights to the next level by adding feature interactions, all crafted and tested in PyTorch. It's like upgrading logistic regression from a solo chess player to a full team working in sync. Testing DINN on synthetic data showed how it can handle complex patterns and uncover hidden insights, leaving plenty of room for future growth and exploration.
This paper goes beyond a new model - it's a flexible framework that mixes regression's simplicity with neural networks' power. I hope it can open up exciting possibilities for machine learning.
